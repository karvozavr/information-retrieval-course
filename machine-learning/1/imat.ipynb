{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imat(method):\n",
    "    method = str(method)\n",
    "    !time java -jar ../../lib/RankLib-2.1-patched.jar -train ../../data/task3_train.txt -tts 0.2 -ranker {method} -metric2t NDCG@20 -sparse -silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [(0, \"MART\"), (1, \"RankNet\"), (2, \"RankBoost\"), (3, \"AdaRank\"), (4, \"Coordinate Ascent\"), (6, \"LambdaMART\"), (7, \"ListNet\"), (8, \"Random Forests\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: MART\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] MART's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "NDCG@20 on test data: 0.7415\n",
      "351.80user 0.96system 4:34.93elapsed 128%CPU (0avgtext+0avgdata 631952maxresident)k\n",
      "0inputs+576outputs (0major+169759minor)pagefaults 0swaps\n",
      "Method: RankNet\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tRankNet\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] RankNet's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "NDCG@20 on test data: 0.6785\n",
      "189.36user 0.42system 3:07.02elapsed 101%CPU (0avgtext+0avgdata 636800maxresident)k\n",
      "0inputs+320outputs (0major+164790minor)pagefaults 0swaps\n",
      "Method: RankBoost\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tRankBoost\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] RankBoost's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "NDCG@20 on test data: 0.7184\n",
      "359.46user 0.68system 5:25.01elapsed 110%CPU (0avgtext+0avgdata 567404maxresident)k\n",
      "0inputs+416outputs (0major+156854minor)pagefaults 0swaps\n",
      "Method: AdaRank\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tAdaRank\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] AdaRank's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "NDCG@20 on test data: 0.7177\n",
      "84.87user 0.29system 1:22.37elapsed 103%CPU (0avgtext+0avgdata 641908maxresident)k\n",
      "0inputs+192outputs (0major+168187minor)pagefaults 0swaps\n",
      "Method: Coordinate Ascent\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tCoordinate Ascent\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] Coordinate Ascent's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "NDCG@20 on test data: 0.73\n",
      "3704.80user 3.27system 1:01:01elapsed 101%CPU (0avgtext+0avgdata 690420maxresident)k\n",
      "0inputs+5024outputs (0major+185221minor)pagefaults 0swaps\n",
      "Method: LambdaMART\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "NDCG@20 on test data: 0.7418\n",
      "399.58user 1.31system 4:51.43elapsed 137%CPU (0avgtext+0avgdata 671944maxresident)k\n",
      "0inputs+480outputs (0major+187796minor)pagefaults 0swaps\n",
      "Method: ListNet\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tListNet\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] ListNet's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "NDCG@20 on test data: 0.7247\n",
      "665.07user 1.82system 10:58.70elapsed 101%CPU (0avgtext+0avgdata 613092maxresident)k\n",
      "72inputs+1168outputs (1major+159403minor)pagefaults 0swaps\n",
      "Method: Random Forests\n",
      "\n",
      "[+] General Parameters:\n",
      "Training data:\t../../data/task3_train.txt\n",
      "Train-Test split: 0.2\n",
      "Feature vector representation: Sparse.\n",
      "Ranking method:\tRandom Forests\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@20\n",
      "Test metric:\tNDCG@20\n",
      "Feature normalization: No\n",
      "\n",
      "[+] Random Forests's Parameters:\n",
      "\n",
      "Reading feature file [../../data/task3_train.txt]... [Done.]            \n",
      "(7033 ranked lists, 75057 entries read)\n",
      "b[1]      | 0.7547    | \n",
      "b[2]      | 0.7488    | \n",
      "b[3]      | 0.7542    | \n",
      "b[4]      | 0.7388    | \n",
      "b[5]      | 0.7387    | \n",
      "b[6]      | 0.7358    | \n",
      "b[7]      | 0.7367    | \n",
      "b[8]      | 0.75      | \n",
      "b[9]      | 0.7373    | \n",
      "b[10]     | 0.7465    | \n",
      "b[11]     | 0.7314    | \n",
      "b[12]     | 0.7328    | \n",
      "b[13]     | 0.7376    | \n",
      "b[14]     | 0.7263    | \n",
      "b[15]     | 0.7498    | \n",
      "b[16]     | 0.7362    | \n",
      "b[17]     | 0.7426    | \n",
      "b[18]     | 0.7398    | \n",
      "b[19]     | 0.7349    | \n",
      "b[20]     | 0.7374    | \n",
      "b[21]     | 0.7371    | \n",
      "b[22]     | 0.7432    | \n",
      "b[23]     | 0.7447    | \n",
      "b[24]     | 0.7414    | \n",
      "b[25]     | 0.7387    | \n",
      "b[26]     | 0.7538    | \n",
      "b[27]     | 0.7352    | \n",
      "b[28]     | 0.7424    | \n",
      "b[29]     | 0.7406    | \n",
      "b[30]     | 0.731     | \n",
      "b[31]     | 0.732     | \n",
      "b[32]     | 0.7433    | \n",
      "b[33]     | 0.738     | \n",
      "b[34]     | 0.7461    | \n",
      "b[35]     | 0.7325    | \n",
      "b[36]     | 0.7386    | \n",
      "b[37]     | 0.734     | \n",
      "b[38]     | 0.7245    | \n",
      "b[39]     | 0.7421    | \n",
      "b[40]     | 0.7382    | \n",
      "b[41]     | 0.7342    | \n",
      "b[42]     | 0.7432    | \n",
      "b[43]     | 0.7384    | \n",
      "b[44]     | 0.7302    | \n",
      "b[45]     | 0.7415    | \n",
      "b[46]     | 0.7469    | \n",
      "b[47]     | 0.7338    | \n",
      "b[48]     | 0.7438    | \n",
      "b[49]     | 0.7321    | \n",
      "b[50]     | 0.7483    | \n",
      "b[51]     | 0.735     | \n",
      "b[52]     | 0.7436    | \n",
      "b[53]     | 0.7458    | \n",
      "b[54]     | 0.7395    | \n",
      "b[55]     | 0.7367    | \n",
      "b[56]     | 0.7523    | \n",
      "b[57]     | 0.7413    | \n",
      "b[58]     | 0.7438    | \n",
      "b[59]     | 0.7413    | \n",
      "b[60]     | 0.7319    | \n",
      "b[61]     | 0.7376    | \n",
      "b[62]     | 0.7228    | \n",
      "b[63]     | 0.7452    | \n",
      "b[64]     | 0.7473    | \n",
      "b[65]     | 0.7435    | \n",
      "b[66]     | 0.725     | \n",
      "b[67]     | 0.7387    | \n",
      "b[68]     | 0.7438    | \n",
      "b[69]     | 0.7375    | \n",
      "b[70]     | 0.7377    | \n",
      "b[71]     | 0.7297    | \n",
      "b[72]     | 0.7265    | \n",
      "b[73]     | 0.7432    | \n",
      "b[74]     | 0.7421    | \n",
      "b[75]     | 0.746     | \n",
      "b[76]     | 0.7469    | \n",
      "b[77]     | 0.7434    | \n",
      "b[78]     | 0.7463    | \n",
      "b[79]     | 0.7392    | \n",
      "b[80]     | 0.7277    | \n",
      "b[81]     | 0.7403    | \n",
      "b[82]     | 0.7365    | \n",
      "b[83]     | 0.7323    | \n",
      "b[84]     | 0.7389    | \n",
      "b[85]     | 0.7559    | \n",
      "b[86]     | 0.7205    | \n",
      "b[87]     | 0.7225    | \n",
      "b[88]     | 0.7376    | \n",
      "b[89]     | 0.7342    | \n",
      "b[90]     | 0.7522    | \n",
      "b[91]     | 0.7308    | \n",
      "b[92]     | 0.7324    | \n",
      "b[93]     | 0.7354    | \n",
      "b[94]     | 0.7265    | \n",
      "b[95]     | 0.7191    | \n",
      "b[96]     | 0.7428    | \n",
      "b[97]     | 0.7383    | \n",
      "b[98]     | 0.7389    | \n",
      "b[99]     | 0.7446    | \n",
      "b[100]    | 0.7432    | \n",
      "b[101]    | 0.7364    | \n",
      "b[102]    | 0.7441    | \n",
      "b[103]    | 0.7446    | \n",
      "b[104]    | 0.7387    | \n",
      "b[105]    | 0.7419    | \n",
      "b[106]    | 0.7512    | \n",
      "b[107]    | 0.7386    | \n",
      "b[108]    | 0.7266    | \n",
      "b[109]    | 0.7197    | \n",
      "b[110]    | 0.7351    | \n",
      "b[111]    | 0.7355    | \n",
      "b[112]    | 0.7295    | \n",
      "b[113]    | 0.7285    | \n",
      "b[114]    | 0.7472    | \n",
      "b[115]    | 0.7411    | \n",
      "b[116]    | 0.757     | \n",
      "b[117]    | 0.7307    | \n",
      "b[118]    | 0.7403    | \n",
      "b[119]    | 0.7361    | \n",
      "b[120]    | 0.7375    | \n",
      "b[121]    | 0.7378    | \n",
      "b[122]    | 0.7351    | \n",
      "b[123]    | 0.7375    | \n",
      "b[124]    | 0.7431    | \n",
      "b[125]    | 0.7382    | \n",
      "b[126]    | 0.7505    | \n",
      "b[127]    | 0.7461    | \n",
      "b[128]    | 0.7243    | \n",
      "b[129]    | 0.7362    | \n",
      "b[130]    | 0.7476    | \n",
      "b[131]    | 0.7441    | \n",
      "b[132]    | 0.7282    | \n",
      "b[133]    | 0.7442    | \n",
      "b[134]    | 0.7431    | \n",
      "b[135]    | 0.7337    | \n",
      "b[136]    | 0.7451    | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b[137]    | 0.7575    | \n",
      "b[138]    | 0.7483    | \n",
      "b[139]    | 0.726     | \n",
      "b[140]    | 0.7414    | \n",
      "b[141]    | 0.726     | \n",
      "b[142]    | 0.7416    | \n",
      "b[143]    | 0.745     | \n",
      "b[144]    | 0.74      | \n",
      "b[145]    | 0.7258    | \n",
      "b[146]    | 0.7376    | \n",
      "b[147]    | 0.7479    | \n",
      "b[148]    | 0.7459    | \n",
      "b[149]    | 0.7345    | \n",
      "b[150]    | 0.747     | \n",
      "b[151]    | 0.7474    | \n",
      "b[152]    | 0.7413    | \n",
      "b[153]    | 0.7519    | \n",
      "b[154]    | 0.7384    | \n",
      "b[155]    | 0.7396    | \n",
      "b[156]    | 0.7515    | \n",
      "b[157]    | 0.7527    | \n",
      "b[158]    | 0.7399    | \n",
      "b[159]    | 0.7432    | \n",
      "b[160]    | 0.7327    | \n",
      "b[161]    | 0.7395    | \n",
      "b[162]    | 0.7336    | \n",
      "b[163]    | 0.7353    | \n",
      "b[164]    | 0.7198    | \n",
      "b[165]    | 0.7408    | \n",
      "b[166]    | 0.745     | \n",
      "b[167]    | 0.7291    | \n",
      "b[168]    | 0.743     | \n",
      "b[169]    | 0.7209    | \n",
      "b[170]    | 0.7412    | \n",
      "b[171]    | 0.7303    | \n",
      "b[172]    | 0.7269    | \n",
      "b[173]    | 0.7418    | \n",
      "b[174]    | 0.7462    | \n",
      "b[175]    | 0.7401    | \n",
      "b[176]    | 0.7435    | \n",
      "b[177]    | 0.7284    | \n",
      "b[178]    | 0.7275    | \n",
      "b[179]    | 0.7369    | \n",
      "b[180]    | 0.7375    | \n",
      "b[181]    | 0.7409    | \n",
      "b[182]    | 0.7506    | \n",
      "b[183]    | 0.7539    | \n",
      "b[184]    | 0.7289    | \n",
      "b[185]    | 0.7253    | \n",
      "b[186]    | 0.7215    | \n",
      "b[187]    | 0.7412    | \n",
      "b[188]    | 0.7507    | \n",
      "b[189]    | 0.7557    | \n",
      "b[190]    | 0.7176    | \n",
      "b[191]    | 0.7543    | \n",
      "b[192]    | 0.7382    | \n",
      "b[193]    | 0.7241    | \n",
      "b[194]    | 0.7488    | \n",
      "b[195]    | 0.7513    | \n",
      "b[196]    | 0.747     | \n",
      "b[197]    | 0.7372    | \n",
      "b[198]    | 0.7386    | \n",
      "b[199]    | 0.7412    | \n",
      "b[200]    | 0.7321    | \n",
      "b[201]    | 0.7356    | \n",
      "b[202]    | 0.7431    | \n",
      "b[203]    | 0.7539    | \n",
      "b[204]    | 0.7262    | \n",
      "b[205]    | 0.7419    | \n",
      "b[206]    | 0.7122    | \n",
      "b[207]    | 0.745     | \n",
      "b[208]    | 0.7488    | \n",
      "b[209]    | 0.7283    | \n",
      "b[210]    | 0.736     | \n",
      "b[211]    | 0.7251    | \n",
      "b[212]    | 0.7204    | \n",
      "b[213]    | 0.7342    | \n",
      "b[214]    | 0.7384    | \n",
      "b[215]    | 0.7332    | \n",
      "b[216]    | 0.7416    | \n",
      "b[217]    | 0.7206    | \n",
      "b[218]    | 0.7308    | \n",
      "b[219]    | 0.7496    | \n",
      "b[220]    | 0.7288    | \n",
      "b[221]    | 0.7304    | \n",
      "b[222]    | 0.7346    | \n",
      "b[223]    | 0.7412    | \n",
      "b[224]    | 0.7343    | \n",
      "b[225]    | 0.7213    | \n",
      "b[226]    | 0.7373    | \n",
      "b[227]    | 0.74      | \n",
      "b[228]    | 0.729     | \n",
      "b[229]    | 0.7386    | \n",
      "b[230]    | 0.7431    | \n",
      "b[231]    | 0.7443    | \n",
      "b[232]    | 0.7403    | \n",
      "b[233]    | 0.7302    | \n",
      "b[234]    | 0.7425    | \n",
      "b[235]    | 0.7391    | \n",
      "b[236]    | 0.7294    | \n",
      "b[237]    | 0.7265    | \n",
      "b[238]    | 0.7474    | \n",
      "b[239]    | 0.736     | \n",
      "b[240]    | 0.7443    | \n",
      "b[241]    | 0.7557    | \n",
      "b[242]    | 0.7328    | \n",
      "b[243]    | 0.7362    | \n",
      "b[244]    | 0.7397    | \n",
      "b[245]    | 0.731     | \n",
      "b[246]    | 0.7259    | \n",
      "b[247]    | 0.7435    | \n",
      "b[248]    | 0.7524    | \n",
      "b[249]    | 0.7489    | \n",
      "b[250]    | 0.7341    | \n",
      "b[251]    | 0.7495    | \n",
      "b[252]    | 0.7425    | \n",
      "b[253]    | 0.7332    | \n",
      "b[254]    | 0.7429    | \n",
      "b[255]    | 0.7251    | \n",
      "b[256]    | 0.7163    | \n",
      "b[257]    | 0.7493    | \n",
      "b[258]    | 0.7411    | \n",
      "b[259]    | 0.7242    | \n",
      "b[260]    | 0.7461    | \n",
      "b[261]    | 0.739     | \n",
      "b[262]    | 0.7286    | \n",
      "b[263]    | 0.7366    | \n",
      "b[264]    | 0.7418    | \n",
      "b[265]    | 0.7353    | \n",
      "b[266]    | 0.7364    | \n",
      "b[267]    | 0.7545    | \n",
      "b[268]    | 0.7403    | \n",
      "b[269]    | 0.736     | \n",
      "b[270]    | 0.7205    | \n",
      "b[271]    | 0.7207    | \n",
      "b[272]    | 0.7413    | \n",
      "b[273]    | 0.7288    | \n",
      "b[274]    | 0.74      | \n",
      "b[275]    | 0.7154    | \n",
      "b[276]    | 0.7462    | \n",
      "b[277]    | 0.7376    | \n",
      "b[278]    | 0.7342    | \n",
      "b[279]    | 0.75      | \n",
      "b[280]    | 0.7465    | \n",
      "b[281]    | 0.7458    | \n",
      "b[282]    | 0.7411    | \n",
      "b[283]    | 0.7429    | \n",
      "b[284]    | 0.7371    | \n",
      "b[285]    | 0.7282    | \n",
      "b[286]    | 0.7417    | \n",
      "b[287]    | 0.7438    | \n",
      "b[288]    | 0.7537    | \n",
      "b[289]    | 0.739     | \n",
      "b[290]    | 0.7546    | \n",
      "b[291]    | 0.7547    | \n",
      "b[292]    | 0.7393    | \n",
      "b[293]    | 0.7471    | \n",
      "b[294]    | 0.7496    | \n",
      "b[295]    | 0.7291    | \n",
      "b[296]    | 0.7284    | \n",
      "b[297]    | 0.731     | \n",
      "b[298]    | 0.748     | \n",
      "b[299]    | 0.7377    | \n",
      "b[300]    | 0.7381    | \n",
      "------------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@20 on training data: 0.7731\n",
      "------------------------------------\n",
      "NDCG@20 on test data: 0.7447\n",
      "1476.39user 27.45system 14:42.81elapsed 170%CPU (0avgtext+0avgdata 525348maxresident)k\n",
      "5248inputs+1768outputs (48major+5792622minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "for method, name in methods:\n",
    "    print(\"Method: \" + name)\n",
    "    train_imat(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
