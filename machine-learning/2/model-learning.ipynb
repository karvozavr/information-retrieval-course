{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/all_info2/\"\n",
    "STATS_FILE = '../../data/stats'\n",
    "TRAIN_FILE = '../../data/train.txt'\n",
    "TEST_FILE = '../../data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_stats(text_dir):\n",
    "    collection = Counter()\n",
    "    collection_doc = Counter()\n",
    "    \n",
    "    for doc in os.listdir(text_dir):\n",
    "        doc = os.path.join(text_dir, doc)\n",
    "\n",
    "        with open(doc) as d:\n",
    "            data = json.load(d)\n",
    "            for document in data:\n",
    "                #document['pagerank'] = get_pagerank_by_url(document[\"url\"])\n",
    "                #yield create_es_action('byweb', document['id'], document)\n",
    "                stem_content = document[\"stem_content\"]\n",
    "                words = list(filter(bool, stem_content.split()))\n",
    "                \n",
    "                collection.update(words)\n",
    "                collection_doc.update(set(words))\n",
    "                    \n",
    "    result = dict()\n",
    "    \n",
    "    for value, count in collection.items():\n",
    "        \n",
    "        result[value] = {\n",
    "            'occurences_total': count, \n",
    "            'occurences_documents': collection_doc[value]\n",
    "        }\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_statistics = get_word_stats(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_ids(text_dir):\n",
    "    collection = Counter()\n",
    "    collection_doc = Counter()\n",
    "\n",
    "    result = dict()\n",
    "    \n",
    "    for doc in os.listdir(text_dir):\n",
    "\n",
    "        doc = os.path.join(text_dir, doc)\n",
    "\n",
    "        with open(doc) as d:\n",
    "            data = json.load(d)\n",
    "            for document in data:\n",
    "                result[document['url']] = document['id']\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids_map = get_document_ids(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701.4231383219044\n"
     ]
    }
   ],
   "source": [
    "def get_avglen(text_dir):\n",
    "    cnt = 0\n",
    "    \n",
    "    for doc in os.listdir(text_dir):\n",
    "\n",
    "        doc = os.path.join(text_dir, doc)\n",
    "\n",
    "        with open(doc) as d:\n",
    "            data = json.load(d)\n",
    "            for document in data:\n",
    "                stem_content = document[\"stem_content\"]\n",
    "                words = list(filter(bool, stem_content.split()))\n",
    "                cnt += len(words)\n",
    "    result = cnt / len(document_ids_map)\n",
    "    return result\n",
    "    \n",
    "avg_len = get_avglen(DATA_DIR)\n",
    "print(avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time\n",
    "from collections import namedtuple\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'queries_xml': '../../data/web2008_adhoc.xml', # path to queries xml\n",
    "    'relevance_2008_xml': '../../data/relevant_table_2008.xml', # path to relevance xml\n",
    "    'relevance_2009_xml': '../../data/relevant_table_2009.xml', # path to relevance xml\n",
    "    'collection_dir': '../../data/all_info2/', # folder with contents of all_info.zip\n",
    "    'pagerank_json' : '../../data/pagerank.json' # json file with pageranks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])\n",
    "if not es.indices.exists(index='byweb'):\n",
    "    es.indices.create(index='byweb')\n",
    "    \n",
    "index_settings = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'content': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'stem_content': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'title': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'id': {\n",
    "                'type': 'keyword'\n",
    "            },\n",
    "            'url': {\n",
    "                'type': 'keyword'\n",
    "            },\n",
    "            'pagerank': {\n",
    "                'type': 'rank_feature'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "       'analysis': {\n",
    "            'analyzer': {\n",
    "                'white_lover': {\n",
    "                    'tokenizer': 'white_20',\n",
    "                    'filter': [\n",
    "                        'lowercase'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'white_20': {\n",
    "                    'type': 'whitespace'\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 1e+03 ns, total: 9 µs\n",
      "Wall time: 11.7 µs\n"
     ]
    }
   ],
   "source": [
    "def recreate_index():\n",
    "    es.indices.delete(index='byweb')\n",
    "    es.indices.create(index='byweb', body=index_settings)\n",
    "    \n",
    "%time\n",
    "recreate_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval of pagerank\n",
    "def get_rank_jsons():\n",
    "    file = settings['pagerank_json']\n",
    "    with open(file) as f:\n",
    "        l = json.load(f)\n",
    "        return dict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_dict = get_rank_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def get_pagerank_by_url(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    # lonely vertex -> pagerank is small\n",
    "    return pagerank_dict.get(domain, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_needed_documents(text_dir, docs):\n",
    "    docs = set(docs)\n",
    "    collection = dict()\n",
    "    \n",
    "    for doc in os.listdir(text_dir):\n",
    "        doc = os.path.join(text_dir, doc)\n",
    "\n",
    "        with open(doc) as d:\n",
    "            data = json.load(d)\n",
    "            for document in data:\n",
    "                if (not document['id'] in docs):\n",
    "                    continue\n",
    "                document['pagerank'] = get_pagerank_by_url(document[\"url\"])\n",
    "                collection[document['id']] = document\n",
    "        \n",
    "    return collection\n",
    "\n",
    "all_collection = get_all_needed_documents(DATA_DIR, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }\n",
    "\n",
    "def es_actions_generator(docs):\n",
    "    for doc in os.listdir(docs):\n",
    "        doc = os.path.join(docs, doc)\n",
    "        with open(doc) as d:\n",
    "            data = json.load(d)\n",
    "            for document in data:\n",
    "                document['pagerank'] = get_pagerank_by_url(document[\"url\"])\n",
    "                yield create_es_action('byweb', document['id'], document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948e7ab059644892b097a19fa0327cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generator = es_actions_generator(settings['collection_dir'])\n",
    "for ok, result in tqdm(parallel_bulk(es, generator, queue_size=4, thread_count=4, chunk_size=1000)):\n",
    "    if not ok:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = namedtuple('Query', ['query_id', 'text', 'relevant', 'not_relevant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query):\n",
    "    text = query.find('{http://www.romip.ru/data/adhoc}querytext').text\n",
    "    query_id = query.attrib['id']\n",
    "    return Query(query_id=query_id, text=text, relevant=[], not_relevant=[])\n",
    "\n",
    "\n",
    "def extract_queries():\n",
    "    filename = settings['queries_xml']\n",
    "    with open(filename) as f:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        tasks = root.findall('{http://www.romip.ru/data/adhoc}task')\n",
    "        qs = list(map(parse_query, tasks))\n",
    "        queries = dict()\n",
    "        for x in qs:\n",
    "            queries[x.query_id] = x\n",
    "        return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevance_2009():\n",
    "    queries_dict = extract_queries()\n",
    "    def parse_relevance(query):\n",
    "        query_id = query.attrib['id']\n",
    "        q = queries_dict[query_id]\n",
    "        for doc in query.findall('./{http://www.romip.ru/common/merged-results}document'):\n",
    "            if (doc.attrib['relevance'] == 'cantbejudged'):\n",
    "                continue # skip\n",
    "            if doc.attrib['relevance'] == 'vital':\n",
    "                q.relevant.append(doc.attrib['id'])\n",
    "            else:\n",
    "                q.not_relevant.append(doc.attrib['id'])\n",
    "        return q\n",
    "\n",
    "    filename = settings['relevance_2009_xml']\n",
    "    with open(filename) as f:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        tasks = root.findall('{http://www.romip.ru/common/merged-results}task')\n",
    "        return list(map(parse_relevance, tasks))\n",
    "    \n",
    "def extract_relevance_2008():\n",
    "    queries_dict = extract_queries()\n",
    "    def parse_relevance(query):\n",
    "        global all_2008_cnt\n",
    "        global not_found_2008_cnt\n",
    "        \n",
    "        query_id = query.attrib['id']\n",
    "        q = queries_dict[query_id]\n",
    "        for doc in query.findall('./{http://www.romip.ru/common/merged-results}document'):\n",
    "            all_2008_cnt = all_2008_cnt + 1\n",
    "            \n",
    "            if (doc.attrib['relevance'] == 'cantbejudged'):\n",
    "                continue # skip\n",
    "            \n",
    "            if (not doc.attrib['id'] in document_ids_map):\n",
    "                not_found_2008_cnt = not_found_2008_cnt + 1\n",
    "                continue\n",
    "            \n",
    "            if doc.attrib['relevance'] == 'notrelevant':\n",
    "                q.not_relevant.append(document_ids_map[doc.attrib['id']])\n",
    "            else:\n",
    "                q.relevant.append(document_ids_map[doc.attrib['id']])\n",
    "        return q\n",
    "\n",
    "    filename = settings['relevance_2008_xml']\n",
    "    with open(filename) as f:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        tasks = root.findall('{http://www.romip.ru/common/merged-results}task')\n",
    "        return list(filter(lambda q: isinstance(q.text, str), map(parse_relevance, tasks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n",
      "419\n",
      "percentile of not found documents in relevance table 2008: 0.15788306764516774\n"
     ]
    }
   ],
   "source": [
    "queries_2009 = extract_relevance_2009()\n",
    "queries_2009 = list(filter(lambda q: q.relevant, queries_2009))\n",
    "print(len(queries_2009))\n",
    "\n",
    "not_found_2008_cnt = 0 \n",
    "all_2008_cnt = 0\n",
    "queries_2008 = extract_relevance_2008()\n",
    "queries_2008 = list(filter(lambda q: q.relevant, queries_2008))\n",
    "print(len(queries_2008))\n",
    "print('percentile of not found documents in relevance table 2008: ' + str(not_found_2008_cnt / all_2008_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, result_size=20):\n",
    "    \"\"\"\n",
    "    Use this function for search.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = {\n",
    "        'query': {\n",
    "             'bool': {\n",
    "                'should': \n",
    "                    {\n",
    "                        'match': {\n",
    "                            'content': query.text.lower()\n",
    "                        }\n",
    "                    }\n",
    "             }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = es.search(index='byweb', body=query, size=result_size)\n",
    "    return list(map(lambda x: x['_id'], result['hits']['hits']))\n",
    "\n",
    "def pretty_print_result(search_result):\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits']:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "                  \n",
    "def get_doc_by_id(doc_id):\n",
    "    return es.get(index='byweb', id=doc_id)['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns [precision, recall, recall-]. Last one is for educational purpose only.\n",
    "def stats_at(k, query, query_result):\n",
    "    relevant = query.relevant\n",
    "    result_at_k = query_result[:k]\n",
    "\n",
    "    good = set(result_at_k).intersection(relevant)\n",
    "    return [len(good) / k, len(good) / len(relevant), len(good) / min(k, len(relevant))]\n",
    "\n",
    "# returns [ap, ap-r, ap-k, ap-]. Last 3 are for educational purpose only\n",
    "def get_ap_at(k, query, query_result):\n",
    "    s = 0\n",
    "    n = 0\n",
    "    relevant = query.relevant\n",
    "    result_at_k = query_result[:k]\n",
    "\n",
    "    for i in range(len(result_at_k)):\n",
    "        result = result_at_k[i]\n",
    "        if result in relevant:\n",
    "            s += stats_at(i + 1, query, result_at_k)[0]\n",
    "            n += 1\n",
    "    \n",
    "    return [s / n if n != 0 else 0, s / len(relevant), s / k, s / min(k, len(relevant))]\n",
    "\n",
    "# returns [map, map-r, map-k, map-]. Last 3 are for educational purpose only\n",
    "def get_map_at(k, queries, q_results):\n",
    "    s1 = 0\n",
    "    s2 = 0\n",
    "    s3 = 0\n",
    "    s4 = 0\n",
    "    for query, query_result in zip(queries, q_results):\n",
    "        ap = get_ap_at(k, query, query_result)\n",
    "        s1 += ap[0]\n",
    "        s2 += ap[1]\n",
    "        s3 += ap[2]\n",
    "        s4 += ap[3]\n",
    "    return [s1 / len(queries), s2 / len(queries), s3 / len(queries), s4 / len(queries)]\n",
    "\n",
    "def get_queries_results(qs, search_fun=search, k=20):\n",
    "    # important: k = max(k, len(q.relevant)), because for r-precision we may need more than k.\n",
    "    return [search_fun(q, max(k, len(q.relevant))) for q in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка для лемматизации запроса; совпадает с лемматизацией для коллекции.\n",
    "\n",
    "stop_words = {'г', '©'}\n",
    "def get_stop_words(files):\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for word in f:\n",
    "                stop_words.add(word.split()[0])\n",
    "\n",
    "get_stop_words(['../../extractor/stopwords/english', '../../extractor/stopwords/russian'])\n",
    "\n",
    "def is_not_stop_word(d):\n",
    "    return not getLexOrText(d) in stop_words\n",
    "\n",
    "\n",
    "normal_word = re.compile('^[A-Za-z0-9Ѐ-ӿ]*$')\n",
    "def is_normal_word(d):\n",
    "    return normal_word.match(d['text']) is not None\n",
    "\n",
    "\n",
    "def getText(d):\n",
    "    return d['text'].lower()\n",
    "\n",
    "def getLexOrText(d):\n",
    "    if 'analysis' not in d or not d['analysis']:\n",
    "        return getText(d)\n",
    " \n",
    "    analysis = d['analysis'][0]\n",
    "    return analysis['lex'] if 'lex' in analysis else getText(d)\n",
    "\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "def lemmatize(query_text):\n",
    "    result = m.analyze(query_text)\n",
    "\n",
    "    result = list(filter(bool, result))\n",
    "    result = list(filter(lambda x: 'analysis' in x or is_normal_word(x), result))\n",
    "    json_results = list(filter(is_not_stop_word, result))\n",
    "\n",
    "    lexed_content = \" \".join(list(map(getLexOrText, json_results)))\n",
    "    return lexed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_search(query, result_size=20):\n",
    "    query_text = lemmatize(query.text)\n",
    "    query = {\n",
    "        'query': {\n",
    "             'bool': {\n",
    "                'should': \n",
    "                    {\n",
    "                        'match': {\n",
    "                            'stem_content': query_text\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    result = es.search(index='byweb', body=query, size=result_size)\n",
    "    return list(map(lambda x: x['_id'], result['hits']['hits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя точность на уровне k=20 по всем запросам: 0.39838383838383895\n",
      "Средняя полнота на уровне k=20 по всем запросам: 0.27127363878837407\n",
      "Средняя полнота-дэш (в знаментеле стоит k, если релевантных документов больше, чем k) на уровне k=20 по всем запросам: 0.468363161597518\n",
      "Средняя R-точность по всем запросам: 0.3389855248333655\n",
      "MAP на уровне k=20: 0.5537058670781846\n",
      "MAP-r (в знаменателе AP - количество всех релевантных документов для запроса) на уровне k=20: 0.1856531340559127\n",
      "MAP-k (в знаменателе AP - k) на уровне k=20: 0.3160643670445948\n",
      "MAP-дэш (в знаменателе AP - k, если релевантных документов больше k, иначе - количество всех релевантных документов для запроса) на уровне k=20: 0.3535702214401734\n"
     ]
    }
   ],
   "source": [
    "get_queries_stats(l_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать поиск по леммам, чтобы доставать возможные документы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_results(qs, search_fun, k=20):\n",
    "    return [search_fun(q, k) for q in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_lemmas(text): # from already stemmed text\n",
    "    return list(filter(bool, text.split()))\n",
    "\n",
    "def tf(text, word_lemma):\n",
    "    lemmas = get_lemmas(text)\n",
    "    cnt = 0\n",
    "    total = len(lemmas)\n",
    "    if (total == 0):\n",
    "        return 0\n",
    "    for lemma in lemmas:\n",
    "        if (lemma == word_lemma):\n",
    "            cnt += 1\n",
    "    return cnt / total\n",
    "\n",
    "def idf(word_lemma):\n",
    "    docs_total = len(document_ids_map)\n",
    "    docs_inside = int(word_statistics[word_lemma]['occurences_documents'])\n",
    "    return np.log(docs_total / docs_inside)\n",
    "\n",
    "def bm25(text, word_lemmas, k1, b):\n",
    "    result = 0\n",
    "    for word_lemma in word_lemmas:\n",
    "        if (not word_lemma in word_statistics):\n",
    "            continue # skip unknown words\n",
    "        the_idf = idf(word_lemma)\n",
    "        the_tf = tf(text, word_lemma)\n",
    "        docs_total = len(document_ids_map)\n",
    "\n",
    "        score = the_idf * the_tf * (k1 + 1)\n",
    "        score = score / (the_tf + k1 * (1 - b + b * docs_total / avg_len))\n",
    "        result += score\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(query_lemmas, text):\n",
    "    text_lemmas = get_lemmas(text)\n",
    "    result = 0.0\n",
    "    for lemma in query_lemmas:\n",
    "        if (lemma in text_lemmas):\n",
    "            result += 1 / len(query_lemmas)\n",
    "    return result\n",
    "\n",
    "def span(query_lemmas, text):\n",
    "    if (len(query_lemmas) == 0):\n",
    "        return 0\n",
    "\n",
    "    text_lemmas = get_lemmas(text)\n",
    "    \n",
    "    if (len(text_lemmas) == 0):\n",
    "        return 0.1\n",
    "    \n",
    "    word_cnts = {}\n",
    "    for word in query_lemmas:\n",
    "        word_cnts[word] = 0\n",
    "\n",
    "    def check_fine():\n",
    "        for key, value in word_cnts.items():\n",
    "            if (value == 0):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    l = 0\n",
    "    min_span = 10 * len(text_lemmas)\n",
    "    for r in range(0, len(text_lemmas)):\n",
    "        word = text_lemmas[r]\n",
    "        if word in word_cnts:\n",
    "            word_cnts[word] += 1\n",
    "            \n",
    "        while (check_fine()):\n",
    "            min_span = min(min_span, r - l + 1)\n",
    "            word = text_lemmas[l]\n",
    "            l += 1\n",
    "            if word in word_cnts:\n",
    "                word_cnts[word] -= 1\n",
    "        \n",
    "    return len(text_lemmas) / min_span\n",
    "\n",
    "def get_vector(document, query_lemmas):\n",
    "    text = document[\"stem_content\"]\n",
    "    title = document[\"title\"]\n",
    "    if (not isinstance(title, str)):\n",
    "        title = \"\"\n",
    "    else:\n",
    "        title = lemmatize(title)\n",
    "    result = []\n",
    "    # bm25\n",
    "    result.append(bm25(text, query_lemmas, 2.0, 0.75))\n",
    "    # normalized bm25\n",
    "    result.append(bm25(text, query_lemmas, 2.0, 0.75) / len(query_lemmas))\n",
    "    # bm25 of title\n",
    "    result.append(bm25(title, query_lemmas, 2.0, 0.75))\n",
    "    # normalized bm25 of title\n",
    "    result.append(bm25(title, query_lemmas, 2.0, 0.75) / len(query_lemmas))\n",
    "    # pagerank\n",
    "    result.append(document['pagerank'])\n",
    "    # query length\n",
    "    result.append(len(query_lemmas))\n",
    "    # document length\n",
    "    result.append(len(get_lemmas(text)))\n",
    "    # url length\n",
    "    result.append(len(document['url']))\n",
    "    # number of references\n",
    "    result.append(len(document['references']))\n",
    "    # query coverage by title\n",
    "    result.append(coverage(query_lemmas, document[\"stem_content\"]))\n",
    "    # query coverage by text\n",
    "    result.append(coverage(query_lemmas, title))\n",
    "    # min span\n",
    "    result.append(span(query_lemmas, document[\"stem_content\"]))\n",
    "    return result\n",
    "\n",
    "def write_vector_data(queries, filename):\n",
    "    needed_docs = []\n",
    "    for query in queries:\n",
    "        needed_docs.extend(query.relevant)\n",
    "        needed_docs.extend(query.not_relevant)\n",
    "    docs = get_all_needed_documents(DATA_DIR, needed_docs)\n",
    "    \n",
    "    qid = 0\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        def write_query(relevance, vector):\n",
    "            line = str(relevance) + ' qid:' + str(qid)\n",
    "            for i, x in enumerate(vector):\n",
    "                line += ' ' + str(i + 1) + \":\" + np.format_float_positional(x, trim='-')\n",
    "            file.write(line + '\\n')\n",
    "        \n",
    "        for query in queries:\n",
    "            qid += 1\n",
    "            query_lemmas = get_lemmas(lemmatize(query.text))\n",
    "            \n",
    "            for doc in query.relevant:\n",
    "                write_query(1, get_vector(docs[doc], query_lemmas))\n",
    "            for doc in query.not_relevant:\n",
    "                write_query(0, get_vector(docs[doc], query_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vector_data(queries_2008, TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test_vector_data(queries, filename):\n",
    "    def is_not_in_2008(query):\n",
    "        for q in queries_2008:\n",
    "            if q.text == query.text:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    queries = list(filter(is_not_in_2008, queries))\n",
    "    \n",
    "    docids = queries_results(queries, l_search, 100)\n",
    "    \n",
    "    for i in range(len(docids)):\n",
    "        docids[i] = list(map(str, docids[i]))\n",
    "        \n",
    "    needed_docs = []\n",
    "    for docss in docids:\n",
    "        needed_docs.extend(docss)\n",
    "    docs = get_all_needed_documents(DATA_DIR, needed_docs)\n",
    "    \n",
    "    qid = 0\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        def write_query(relevance, vector):\n",
    "            line = str(relevance) + ' qid:' + str(qid)\n",
    "            for i, x in enumerate(vector):\n",
    "                line += ' ' + str(i + 1) + \":\" + np.format_float_positional(x, trim='-')\n",
    "            file.write(line + '\\n')\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            qid += 1\n",
    "            query_lemmas = get_lemmas(lemmatize(query.text))\n",
    "            \n",
    "            docss = docids[i]\n",
    "            \n",
    "            for docid in docss:\n",
    "                if (docid in query.relevant):\n",
    "                    relevance = 1\n",
    "                else:\n",
    "                    relevance = 0\n",
    "                write_query(relevance, get_vector(docs[docid], query_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_test_vector_data(queries_2009, TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar ../../lib/RankLib-2.12.jar -load mart.txt -test ../../data/test.txt -ranker 0 -metric2t NDCG@20 -norm linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "значение группы грови и резус фактор при беремонности\n",
      "номос\n",
      "edo бумага\n",
      "рустам солнцев\n",
      "мукоза\n",
      "Следует однако отметить\n",
      "СТИХИ ТОЛЬКО ЧТО РОДИВШИМСЯ МАЛЬЧИКАМ\n",
      "эротика\n",
      "НОРМЫ РАСХОДА\n",
      "список всех коммерчиских и государственных\n",
      "квартирант\n",
      "Поковки стальные штамповочные ГОСТ 1505-89\n",
      "Воротынские унитазы отзывы\n",
      "сурдология\n",
      "фото голых малолеток\n",
      "что такое торрент файлы\n",
      "велком\n",
      "незаконное проживание мигрантов\n",
      "что положено вдове военнослужащего пенсионера список документов\n",
      "автозапчасти на ваз 2105\n",
      "Бесплатная загрузка фотографий\n",
      "база серебряное копытце\n",
      "APE TO MP3\n",
      "печать свадебных книг\n",
      "международня логистика и ее особенности\n",
      "карта москвы\n",
      "ОАО Отечественные лекарства ,\n",
      "Honda XR650R XR 650 R SM\n",
      "административное право как отрасль права\n",
      "франция румыния евро-2008\n",
      "обучение английскому языку на мальте\n",
      "заказ билетов\n",
      "color night скачать музыка\n",
      "реферат История Конституцияя РФ\n",
      "авангард\n",
      "книга кэттролл дом книги\n",
      "праздники\n",
      "цирк ШАПИТО\n",
      "Автосигнализация Mongoose CYBORG\n",
      "расхода газовоздушной смеси автомобиля\n",
      "коды к играм\n",
      "спортмастер\n",
      "купить LCD Samsung 741MP\n",
      "инструкция для Olympus Camedia C55 Zoom\n",
      "Москвич 412\n",
      "Сиофор\n",
      "скачать песню Серёга отчего\n",
      "Грузоподъемность автомобиля как вычислить\n",
      "меняем лого с помощью CSS\n",
      "домашние кинотеатры\n",
      "6 типография\n",
      "ночевать mazda 3 разложить сидения\n",
      "Ленинградская область\n",
      "индекс Лернера\n",
      "проверка загружаемого изображения на сайт php\n",
      "выписка из Егрюл\n",
      "Погода евпатория\n",
      "одесса\n",
      "Мастер производственного обучения.ru Методические разработки по специальности Повар\n",
      "камины печи\n",
      "loveplanet\n",
      "Хентай наруто\n",
      "абзац\n",
      "Ира Ортман\n",
      "керимов\n",
      "взлом wi-fi сетей\n",
      "Color Pilot\n",
      "рефлексотерапия\n",
      "Религии в России статистика\n",
      "19 Кинотавр 2008 Участники\n",
      "skypemate\n",
      "сделать тент для пляжа\n",
      "virtual dub\n",
      "ищу работу\n",
      "официантки\n",
      "удаление камней из почек\n",
      "УГТУ -УПИ Екатеринбург официальный сайт\n",
      "getwindowthreadprocessid\n",
      "кудымкар\n",
      "обществознание 9 класс билеты и ответы\n",
      "volzwagen\n",
      "beauty редактор\n",
      "мебельные фабрики Санкт Петербурга\n",
      "когда на волгу шаровую подвеску\n",
      "золотая звезда\n",
      "аудиокнига-над пропастью во ржи\n",
      "тополинный пух жара\n",
      "2006 год пойкио\n",
      "инвестор\n",
      "РЕЗИНОВОЕ ПОКРЫТИЕ\n",
      "ноги в сапогах отпотевают\n",
      "Билан\n",
      "эротика\n",
      "церковь дом отца\n",
      "счет 01\n",
      "какая из термоядерных реакций наиболее важна для подержания светимости солнца,\n",
      "язык и культура\n",
      "египетский бог искусств\n",
      "tut.by\n",
      "семейный отдых в анапе\n",
      "правила общения вожатого с детьми\n",
      "норма триады гормонов материнской сыворотки\n",
      "проблемы с мобильной icq\n",
      "Российская межрегиональная ассоциация предпринимателей текстильной и легкой промышленности\n",
      "Николай 1\n",
      "типография фора-трафорет\n",
      "Яцуба Виктор Васильевич\n",
      "ключи к играм mail.ru\n",
      "Юридическое консультирование как вид социальной помощи\n",
      "открытка с днем рождения\n",
      "Мойки Кухонные Franke\n",
      "антивирус скачать\n",
      "фильм посредник\n",
      ". Мероприятия к дню рождения Петра - 1, для 3, 4, 5 класссов\n",
      "фильм гарри потер и принц полукровка\n",
      "иконки\n",
      "тринити моторс\n",
      "аналог википедии\n",
      "спорт оптика на ваз 2109\n",
      "институты финансового права классификация реферат скачать\n",
      "удерживается ли с больничного листа НДФЛ?\n",
      "антенна кавказ продажа\n",
      "гиперактивность младенца\n",
      "Эрих Мария Ремарк\n",
      "реферат электроная почта\n",
      "скальпинг\n",
      "билеты от киева до луска\n",
      "диод справочник\n",
      "зайцев нет\n",
      "диплом титульный лист\n",
      "portable софт\n",
      "мальдивы ковровое покрытие\n",
      "гениалогия\n",
      "бесплатная регистрация в катологах\n",
      "Найк Борзов\n",
      "требования ЕЭК ООН\n",
      "игры для двоих\n",
      "Warsow\n",
      "жк телевизоры\n",
      "key-UserGate-4.2.0.3459.rar\n",
      "конструкция чип конденсатора\n",
      "дорожная карта\n",
      "детский бромгексин\n",
      "Предохранители всё о\n",
      "коробка управления\n",
      "как сиправить пордок следования текста в icq\n",
      "доминикана отзывы\n",
      "РЖД\n",
      "спортивное питание в спб\n",
      "Еврокар\n",
      "Задний фонарь на bmw e28\n",
      "mail агент\n",
      "Гельветика\n",
      "эргометр rx7 7686-000\n",
      "резюме чекменева маргарита федоровна\n",
      "Печатник.сом\n",
      "ежевика\n",
      "переноска для перевозки собак\n",
      "усилитель сигнала для nokia 2630\n",
      "аренда офиса волгодонск\n",
      "ГОСТ 6.30-2003\n",
      "Навигатор для Новосибирска\n",
      "как заполнять книгу учета доходов и расходов\n",
      "строительство склада типовой проект\n",
      "кошка\n",
      "millennium-2000\n",
      "индийские песни\n",
      "чистая ссудная задолженность\n",
      "ДОЛ МАЯК\n",
      "госжилинспекция\n",
      "марафон\n",
      "Аватар аниме\n",
      "Статья 19 Закон о милиции\n",
      "медицинские услуги в Кунцевском районе\n",
      "LG Flatron 1953S\n",
      "интернет скейт шоп балетки\n",
      "продажа тульская область дом егнышевка\n",
      "компьютер\n",
      "санаторий голубой залив\n",
      "ПРОШИВКА НА ГЛОУФИШ Х800 ПОСЛЕДНЯЯ РАБОЧАЯ ВЕРСИЯ\n",
      "Слоник Рок-н-ролл CHICCO\n",
      "наличник\n",
      "побег из тюрьмы 3\n",
      "гипермаркет столица ижевск\n",
      "встречное исковое заявление сносе торгового павильона  на землях общего\n",
      "песня led zeppelin из рекламы\n",
      "Система вентиляции и кондиционирования продажа\n",
      "бот для чата\n",
      "гипнотерапия\n",
      "факторы внутренней среды\n",
      "Кама банк\n",
      "специализация менеджмента в туризме скачать\n",
      "прыгай внис ноты фортепиано\n",
      "рбота в новосибирске vip\n",
      "новости шоу бизнеса\n",
      "Музыка для Sony Ericsson\n",
      "история старой ладоги\n",
      "авторевю\n",
      "гадание онлайн\n",
      "водительская медкомиссия адреса\n",
      "Трансконтейнер\n",
      "мебель vox\n",
      "работа для студентов\n",
      "Как настроить аську?\n",
      "calve\n",
      "автобусы брянск бежица\n",
      "МУССОЛИНИ\n",
      "Скачать USDownloader 1.3.4.8\n",
      "сообщение об открытии счета скачать\n",
      "Лейтмотив искушений Бесприданница курсовая\n",
      "MTA AS267N Чиллер холодопроивод. 267 кВт, фреон R407C\n",
      "Восстания 1\n",
      "тюненг ВАЗ 2112\n",
      "трейнинг тест по икб\n",
      "прогноз погоды\n",
      "таблица пар трения\n",
      "прогноз погоды\n",
      "в каком сборнике монтаж анализатора метана\n",
      "вольная борьба\n",
      "знаменитости\n",
      "реферат государственное регулирование цен в россии\n",
      "Возникновение философии.\n",
      "строительство лестниц\n",
      "Реферат по Всемирной истории на тему: Советский тыл в годы Великой Отечественной войны\n",
      "НАБЕРЕЖНЫЕ ЧЕЛНЫ\n",
      "киндер\n",
      "Павел Вольский\n",
      "Кижи\n",
      "роснедвижимость\n",
      "разработка\n",
      "взлом сайтов на Joomla\n",
      "телефон Русь 26 ремонт немнабирает номер\n",
      "Горбушка\n",
      "музей работает допоздна\n",
      "Армянское общение\n",
      "катание на гоночных машинах\n",
      "автосалон\n",
      "калужское управление лесами\n",
      "Full Moon Party\n",
      "город тур франция\n",
      "Крепеж\n",
      "картинки аниме\n",
      "москва\n",
      "Вестерн Юнион\n",
      "пилон\n",
      "йодантипирин\n",
      "freebsd обновить пути\n",
      "темы и игры на телефон\n",
      "карта Севастополя\n",
      "скачать ответы на билеты по химии профильного уровня за 11 класс\n",
      "анализ безубыточности хлеба\n",
      "воздухоопорные конструкции\n",
      "купить квартиру в валенсии\n",
      "сочинение по Война и мир\n",
      "что такое личный доход?\n",
      "дробеструйная обработка\n",
      "допуски и посадки\n",
      "2n3904 аналог\n",
      "нож охотничий\n",
      "Условия и факторы качества решений\n",
      "Мэрилин Монро\n",
      "автоцентр кунцево\n",
      "срок действия амортизационной премии\n",
      "ответы на вопросы по правоохранительным органам\n",
      "госэкзамен шпоргалки\n",
      "шум. Измерение и нормирование. Защита работающих от воздействия шума\n",
      "значение имён\n",
      "СТОИМОСТЬ БИЛЕТА МИНСК-ОДЕССА\n",
      "тепловой расчет печи\n",
      "гороскоп\n",
      "Olympus SP-510 Ultra Zoom\n",
      "вывоз строительного мусора\n",
      "продажа билетов на Санкт-петербург Ленинградский вокзал\n",
      "Природа, сущность и общественная роль политологии.\n",
      "полиграфические услуги\n",
      "ткань таффета\n",
      "бетховен\n",
      "Гороскоп\n",
      "lexus gs300\n",
      "русский тур\n",
      "определения управление\n",
      "межрегионгаз\n",
      "магазины для беременных\n",
      "зарплатомер\n",
      "Хорватия\n",
      "спорт экспресс\n",
      "вредна ли гарнитура bluetooth\n",
      "втб24\n",
      "WAB импорт в MS Outlook\n",
      "Палочкоядерные нейтрофилы\n",
      "философская характеристика русской идеи как духовного образования XIXв. искать\n",
      "памятник пушкину в эфиопии фото\n",
      "бесплодные усилия\n",
      "Вредители малины\n",
      "лесные птицы\n",
      "сукуба\n",
      "замена колодок Mazda3\n",
      "беременность\n",
      "chrysler 300c voyager\n",
      "минимакс\n",
      "контрольные работы\n",
      "игры он лайн\n",
      "сайт  Сделай сам\n",
      "отели Севастополя\n",
      "терапевт Парвенкова\n",
      "Бензогенератор инверторного типа KIPOR IG1000 (кожух)\n",
      "курантил\n",
      "привлекательнее\n",
      "прога для Nokia N70\n",
      "втб 24\n",
      "Проверка на невидимость в ICQ\n",
      "рога и копыта Золотой телёнок\n",
      "порашковые краски\n",
      "Золотой ус против Витилиго\n",
      "ГОСТ19903\n",
      "Ryobi\n",
      "Собрать аську на телефон\n",
      "фронда мебель\n",
      "Настройки аськи для кпк Мегафон\n",
      "Motorola форум\n",
      "пробки\n",
      "матрас 140х200\n",
      "nissan cashkay\n",
      "болонки\n",
      "ДДТ на Бебеля\n",
      "В каком году Ники Лауда начал свою карьеру в команде Scuderia Ferrari ?\n",
      "тренинги Арсенал\n",
      "пособие по уходу за ребенком\n",
      "дозировка хлорфилиппта\n",
      "заказ пиццы по Ташкенту\n",
      "административный кодекс р б\n",
      "отзывы citroen xsara\n",
      "Вита теплоход\n",
      "благотворительный фонд АиФ. Доброе сердце\n",
      "правительство Республики Башкортостан\n",
      "В каком банке можно взять кредит под залог недвижимости\n",
      "МТС\n",
      "скачать образцы исковых заявлений о заявление о права собственности на земельный участок в силу приобретательной давности\n",
      "все для туризма\n",
      "философские кроссворды\n",
      "фурнитура окон schueco\n",
      "моя прекрасная няня\n",
      "Александр Домагаров\n",
      "struct C\n",
      "Казан, мангал и другие мужские удовольствия\n",
      "абажур\n",
      "Ростэк кубань\n",
      "форд фокус\n",
      "Росбанк представительство в Чебоксарах\n",
      "поезда коммерческие кассы\n",
      "Настя Задорожная\n",
      "лицей 1535\n",
      "ж.д билеты\n",
      "поликлиники ЦК отзывы\n",
      "pavilion dv9595\n",
      "cdr\n",
      "европа плюс\n",
      "Письмо Минфина РФ от 11.04.2008г. №03-11-04/3/187\n",
      "Быстрянский биография\n",
      "арбат престиж\n",
      "дельфин группа\n",
      "получить статистику по дискам Linux\n",
      "комплекс определения вязкости растворов в широком диапазоне температур 1-100 мпа с\n",
      "except перевод\n",
      "Путешествие во Вьетнам\n",
      "договор сотрудничества с риэлтрором\n",
      "землетрясение в Греции 6,5\n",
      "последствия кодировки от алкогализма\n",
      "качественное удостоверение на продукты питания\n",
      "кондиционер midea\n",
      "сеть магазинов обуви\n",
      "ванга\n",
      "Дэниел Колт ubs\n",
      "туристическая компания Пегас\n",
      "Как правильно оформить платежное поручение по налогу на прибыль с дивидендов у Российской организации\n",
      "цены фольсваген\n",
      "капель\n",
      "смартс\n",
      "облицовачный кирпич\n",
      "форекс клуб\n",
      "погода в Сухуми\n",
      "пожелание для неё спокойной ночи\n",
      "горячие туры\n",
      "зайцев нет\n",
      "Щуп складской для отбора проб\n",
      "доставка суши в москве\n",
      "новая поисковая система\n",
      "игра охота\n",
      "пограничная академия ФСБ\n",
      "Керабуд\n",
      "авто\n",
      "сестринский процесс при инфаркте миокарда\n",
      "термин ambient\n",
      "Lanmaster прайс\n",
      "Новый ИЖ - 5\n",
      "русфинанс банк самара\n",
      "монстры\n",
      "безопасность жизнедеятельности\n",
      "харьков\n",
      "женская одежда магазин\n",
      "разновидности логотипов\n",
      "голова Бафомета\n",
      "собаки маленькие\n",
      "канцелярские товары в ярославле\n",
      "детская кровать с каким маятником\n",
      "Тунис\n",
      "см клиника\n",
      "банк Возрождение в Санкт-Петербурге отдел персонала\n",
      "песня И не ценишь что даю тебе я, и берешь у других, что есть у меня\n",
      "первая игра на троих для iPhone\n",
      "трехмерное маделирование по фотографии\n",
      "оборудование для станкостроения мойка\n",
      "группа земфира\n",
      "Battlestar Galactica\n",
      "расчет параметров асинхронного двигателя\n",
      "расписание поездов - Казанский вокзал\n",
      "астра авто\n",
      "СНиП 3-02.01-87\n",
      "Скачать билеты по ОБЖ 2208\n"
     ]
    }
   ],
   "source": [
    "for query in queries_2008:\n",
    "    print(query.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jamspell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9c6b24a55ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjamspell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jamspell'"
     ]
    }
   ],
   "source": [
    "import jamspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
