{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'queries_xml': '/home/subject/Downloads/web2008_adhoc.xml', # path to queries xml\n",
    "    'relevance_xml': '/home/subject/Downloads/or_relevant-minus_table.xml', # path to relevance xml\n",
    "    'collection_dir': '/home/subject/Documents/informational retrieval/all_info2/', # folder with contents of all_info.zip\n",
    "    'pagerank_json' : '/home/subject/Documents/informational retrieval/pagerank.json' # json file with pageranks\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not es.indices.exists(index='byweb'):\n",
    "    es.indices.create(index='byweb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'content': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'stem_content': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'title': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'id': {\n",
    "                'type': 'keyword'\n",
    "            },\n",
    "            'url': {\n",
    "                'type': 'keyword'\n",
    "            },\n",
    "            'pagerank': {\n",
    "                'type': 'rank_feature'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "       'analysis': {\n",
    "            'analyzer': {\n",
    "                'white_lover': {\n",
    "                    'tokenizer': 'white_20',\n",
    "                    'filter': [\n",
    "                        'lowercase'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'white_20': {\n",
    "                    'type': 'whitespace'\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_index():\n",
    "    es.indices.delete(index='byweb')\n",
    "    es.indices.create(index='byweb', body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "recreate_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval of pagerank\n",
    "def get_rank_jsons():\n",
    "    file = settings['pagerank_json']\n",
    "    with open(file) as f:\n",
    "        l = json.load(f)\n",
    "        return dict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_dict = get_rank_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def get_pagerank_by_url(pagerank_dict, page_json):\n",
    "    domain = f'{urlparse(page_json[\"url\"]).netloc}'\n",
    "    # lonely vertex -> pagerank is 0\n",
    "    return pagerank_dict.get(domain, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }\n",
    "\n",
    "def es_actions_generator(docs):\n",
    "    for doc in os.listdir(docs):\n",
    "        doc = os.path.join(docs, doc)\n",
    "        with open(doc) as d:\n",
    "            data = json.load(d)\n",
    "            for document in data:\n",
    "                #   document['pagerank'] = get_pagerank_by_url(pagerank_dict, document)\n",
    "                yield create_es_action('byweb', document['id'], document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1fa2a4d82448c281100d8a51bc8269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = es_actions_generator(settings['collection_dir'])\n",
    "for ok, result in tqdm(parallel_bulk(es, generator, queue_size=4, thread_count=4, chunk_size=1000)):\n",
    "    if not ok:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время работы процедуры выше: чуть больше 5 минут.\n",
    "\n",
    "Если посмотрим сюда: \"http://localhost:9200/_stats/indexing,store?pretty \" и найдем:\n",
    "\n",
    "`indices.byweb.total.store.size_in_bytes`\n",
    "\n",
    "увидим, что размер индекса равен ~3 Гб. Размер информации, запихнутой в индекс, ~4.1 Гб.\n",
    "\n",
    "(Конечно, все это можно было сделать программно, но посмотреть на один показатель быстрее ручками.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = namedtuple('Query', ['query_id', 'text', 'relevant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query):\n",
    "    text = query.find('{http://www.romip.ru/data/adhoc}querytext').text\n",
    "    query_id = query.attrib['id']\n",
    "    return Query(query_id=query_id, text=text, relevant=[])\n",
    "\n",
    "\n",
    "def extract_queries():\n",
    "    filename = settings['queries_xml']\n",
    "    with open(filename) as f:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        tasks = root.findall('{http://www.romip.ru/data/adhoc}task')\n",
    "        qs = list(map(parse_query, tasks))\n",
    "        queries = dict()\n",
    "        for x in qs:\n",
    "            queries[x.query_id] = x\n",
    "        return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevance():\n",
    "    queries_dict = extract_queries()\n",
    "    def parse_relevance(query):\n",
    "        query_id = query.attrib['id']\n",
    "        q = queries_dict[query_id]\n",
    "        for doc in query.findall('./{http://www.romip.ru/common/merged-results}document'):\n",
    "            if doc.attrib['relevance'] == 'vital':\n",
    "                q.relevant.append(doc.attrib['id'])\n",
    "        return q\n",
    "\n",
    "    filename = settings['relevance_xml']\n",
    "    with open(filename) as f:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        tasks = root.findall('{http://www.romip.ru/common/merged-results}task')\n",
    "        return list(map(parse_relevance, tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = extract_relevance()\n",
    "# important: -38 queries\n",
    "queries = list(filter(lambda q: q.relevant, queries))\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, result_size=20):\n",
    "    \"\"\"\n",
    "    Use this function for search.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = {\n",
    "        'query': {\n",
    "             'bool': {\n",
    "                'should': \n",
    "                    {\n",
    "                        'match': {\n",
    "                            'content': query.text\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = es.search(index='byweb', body=query, size=result_size)\n",
    "    return list(map(lambda x: x['_id'], result['hits']['hits']))\n",
    "\n",
    "def pretty_print_result(search_result):\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits']:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "                  \n",
    "def get_doc_by_id(doc_id):\n",
    "    return es.get(index='byweb', id=doc_id)['_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистические показатели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_at(k, query, query_result):\n",
    "    relevant = query.relevant\n",
    "    result_at_k = query_result[:k]\n",
    "    good = set(result_at_k).intersection(relevant)\n",
    "    return [len(good) / k, len(good) / len(relevant), len(good) / min(k, len(relevant))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap_at(k, query, query_result):\n",
    "    s = 0\n",
    "    n = 0\n",
    "    for i in range(len(query_result)):\n",
    "        result = query_result[i]\n",
    "        if result in query.relevant:\n",
    "            s += stats_at(i + 1, query, query_result)[0]\n",
    "            n += 1\n",
    "    return [0 if n == 0 else s / n, s / len(query.relevant), s / min(k, len(query.relevant))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_at(k, queries, q_results):\n",
    "    s1 = 0\n",
    "    s2 = 0\n",
    "    s3 = 0\n",
    "    for query, query_result in zip(queries, q_results):\n",
    "        ap = get_ap_at(k, query, query_result)\n",
    "        s1 += ap[0]\n",
    "        s2 += ap[1]\n",
    "        s3 += ap[2]\n",
    "    return [s1 / len(queries), s2 / len(queries), s3 / len(queries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_results(qs, search_fun=search, k=20):\n",
    "    return [search_fun(q, k) for q in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_stats(search_fun, k=20):\n",
    "    results = get_queries_results(queries, search_fun, k)\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    recall_dash = 0\n",
    "    r_precision = 0\n",
    "    for q, r in zip(queries, results):\n",
    "        stats = stats_at(k, q, r)\n",
    "        precision += stats[0]\n",
    "        recall += stats[1]\n",
    "        recall_dash += stats[2]\n",
    "        r_precision += stats_at(len(q.relevant), q, r)[0]\n",
    "    \n",
    "    queries_size = len(queries)\n",
    "    print(\"Средняя точность на уровне k=20 по всем запросам:\", precision / queries_size)\n",
    "    print(\"Средняя полнота на уровне k=20 по всем запросам:\", recall / queries_size)\n",
    "    print(\"Средняя полнота-дэш (в знаментеле стоит k, если релевантных документов больше, чем k) на уровне k=20 по всем запросам:\", recall_dash / queries_size)\n",
    "    print(\"Средняя R-точность на уровне k=20 по всем запросам:\", r_precision / queries_size)\n",
    "    map_at = get_map_at(k, queries, results)\n",
    "    print(\"MAP на уровне k=20:\", map_at[0])\n",
    "    print(\"MAP-дэш (в знаменателе AP - количество всех релевантных документов для запроса) на уровне k=20:\", map_at[1])\n",
    "    print(\"MAP-дэш-дэш (в знаменателе AP - k, если релевантных документов больше k, иначе - количество всех релевантных документов для запроса) на уровне k=20:\", map_at[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_queries_stats(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "_ = get_queries_results(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметки:\n",
    "\n",
    "Query(query_id='arw53730', text='Ремолан', relevant=['980017']) []\n",
    "потому что mystem, ремолать\n",
    "\n",
    "Зачем нужны дэш-характеристики:\n",
    "\n",
    "Query(query_id='arw50384', text='гизметео', relevant=['587130', '856298', '239338', '239387', '239377', '687059', '261338', '325277', '72835', '1285231', '223037', '1382550', '37733', '92274', '92282', '1420731', '689932', '1303050', '963503', '602944', '1372162', '256386', '1427662', '1464330', '1357789']) ['1420731', '691030'] 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка для лемматизации запроса; совпадает с лемматизацией для коллекции.\n",
    "\n",
    "stop_words = {'г', '©'}\n",
    "def get_stop_words(files):\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for word in f:\n",
    "                stop_words.add(word.split()[0])\n",
    "\n",
    "get_stop_words(['../extractor/stopwords/english', '../extractor/stopwords/russian'])\n",
    "\n",
    "def is_not_stop_word(d):\n",
    "    return not getLexOrText(d) in stop_words\n",
    "\n",
    "\n",
    "normal_word = re.compile('^[A-Za-z0-9Ѐ-ӿ]*$')\n",
    "def is_normal_word(d):\n",
    "    return normal_word.match(d['text']) is not None\n",
    "\n",
    "\n",
    "def getText(d):\n",
    "    return d['text'].lower()\n",
    "\n",
    "def getLexOrText(d):\n",
    "    if 'analysis' not in d or not d['analysis']:\n",
    "        return getText(d)\n",
    " \n",
    "    analysis = d['analysis'][0]\n",
    "    return analysis['lex'] if 'lex' in analysis else getText(d)\n",
    "\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "def lemmatize(query_text):\n",
    "    result = m.analyze(query_text)\n",
    "\n",
    "    result = list(filter(bool, result))\n",
    "    result = list(filter(lambda x: 'analysis' in x or is_normal_word(x), result))\n",
    "    json_results = list(filter(is_not_stop_word, result))\n",
    "\n",
    "    lexed_content = \" \".join(list(map(getLexOrText, json_results)))\n",
    "    return lexed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_search(query, result_size=20):\n",
    "    query_text = lemmatize(query.text)\n",
    "    query = {\n",
    "        'query': {\n",
    "             'bool': {\n",
    "                'should': \n",
    "                    {\n",
    "                        'match': {\n",
    "                            'stem_content': query_text\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    result = es.search(index='byweb', body=query, size=result_size)\n",
    "    return list(map(lambda x: x['_id'], result['hits']['hits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_queries_stats(l_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "_ = get_queries_results(queries, l_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_search(query, result_size=20):\n",
    "    query = {\n",
    "        'query': {\n",
    "             'bool': {\n",
    "                'should': [\n",
    "                    {\n",
    "                        'match': {\n",
    "                            'stem_content': query.text\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'rank_feature': {\n",
    "                            'field': 'pagerank',\n",
    "                            'saturation': {\n",
    "                                'pivot': 10\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = es.search(index='byweb', body=query, size=result_size)\n",
    "    return list(map(lambda x: x['_id'], result['hits']['hits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: учитывать title в запросе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
